"""
Demostraci√≥n de uso de ProblemFactory.

Este m√≥dulo muestra c√≥mo usar la factory para generar problemas
de forma automatizada y ejecutar benchmarks masivos.
"""
import pytest
from .factory import (
    ProblemFactory,
    ProblemGenerator,
    ProblemConfig,
    ProblemType,
    quick_problem,
    batch_problems,
    scalability_suite,
    get_quick_suite,
    get_nqueens_suite,
    get_comparison_suite,
)
from .algorithms import get_solver
from .runner import BenchmarkRunner


@pytest.mark.benchmark
class TestProblemFactory:
    """Tests de demostraci√≥n de ProblemFactory."""
    
    def test_quick_problem_creation(self):
        """
        Demo: Creaci√≥n r√°pida de problemas individuales.
        """
        print("\n" + "="*70)
        print("  DEMO: Creaci√≥n R√°pida de Problemas")
        print("="*70)
        
        # Crear problemas individuales
        problems = [
            quick_problem("nqueens", 6),
            quick_problem("nqueens", 8),
            quick_problem("graph_coloring", 10, difficulty="medium"),
            quick_problem("scheduling", 7, difficulty="easy"),
        ]
        
        print(f"\n‚úÖ Creados {len(problems)} problemas:")
        for p in problems:
            print(f"  - {p.name} [{p.difficulty}]")
        
        assert len(problems) == 4
        assert all(p.problem is not None for p in problems)
    
    def test_batch_creation(self):
        """
        Demo: Creaci√≥n en batch de m√∫ltiples problemas.
        """
        print("\n" + "="*70)
        print("  DEMO: Creaci√≥n en Batch")
        print("="*70)
        
        # Crear batch de N-Reinas
        sizes = [4, 6, 8, 10, 12]
        problems = batch_problems("nqueens", sizes)
        
        print(f"\n‚úÖ Creados {len(problems)} problemas N-Reinas:")
        for p in problems:
            print(f"  - {p.name}")
        
        assert len(problems) == len(sizes)
    
    def test_scalability_suite(self):
        """
        Demo: Suite para an√°lisis de escalabilidad.
        """
        print("\n" + "="*70)
        print("  DEMO: Suite de Escalabilidad")
        print("="*70)
        
        # Crear suite de escalabilidad
        problems = scalability_suite("nqueens", min_size=4, max_size=10, step=2)
        
        print(f"\n‚úÖ Suite de escalabilidad (step=2):")
        for p in problems:
            print(f"  - {p.name}")
        
        assert len(problems) == 4  # [4, 6, 8, 10]
    
    def test_predefined_suites(self):
        """
        Demo: Suites predefinidas.
        """
        print("\n" + "="*70)
        print("  DEMO: Suites Predefinidas")
        print("="*70)
        
        # Suite r√°pida
        quick = get_quick_suite()
        print(f"\n‚úÖ Suite R√°pida ({len(quick)} problemas):")
        for p in quick:
            print(f"  - {p.name} [{p.difficulty}]")
        
        # Suite de N-Reinas
        nqueens = get_nqueens_suite(max_n=8)
        print(f"\n‚úÖ Suite N-Reinas ({len(nqueens)} problemas):")
        for p in nqueens:
            print(f"  - {p.name}")
        
        # Suite de comparaci√≥n
        comparison = get_comparison_suite()
        print(f"\n‚úÖ Suite de Comparaci√≥n ({len(comparison)} categor√≠as):")
        for category, probs in comparison.items():
            print(f"  - {category}: {len(probs)} problemas")
        
        assert len(quick) == 5
        assert len(nqueens) == 5  # [4, 5, 6, 7, 8]
        assert len(comparison) == 3  # small, medium, large
    
    def test_automated_benchmark_with_factory(self):
        """
        Demo: Benchmark automatizado usando factory.
        
        Genera problemas autom√°ticamente y ejecuta benchmarks.
        """
        print("\n" + "="*70)
        print("  DEMO: Benchmark Automatizado con Factory")
        print("="*70)
        
        # Generar problemas autom√°ticamente
        problems = batch_problems("nqueens", [4, 6, 8])
        
        # Configurar runner
        runner = BenchmarkRunner(warmup_runs=0, benchmark_runs=3)
        
        # Algoritmo a testear
        solver = get_solver("backtracking")
        
        print(f"\n{'Problema':<20} {'Tiempo (ms)':<15} {'Nodos':<10}")
        print("-"*50)
        
        # Ejecutar benchmarks
        for problem in problems:
            result = runner.run_benchmark(problem, solver, "Backtracking", max_solutions=1)
            
            print(f"{problem.name:<20} "
                  f"{result.time_mean_ms:>7.2f} ¬± {result.time_std_ms:<5.2f} "
                  f"{result.nodes_mean:>9.0f}")
            
            assert result.success_rate == 1.0
            assert result.solutions_found > 0
        
        print("\n‚úÖ Benchmark automatizado completado")
    
    def test_massive_suite_generation(self):
        """
        Demo: Generaci√≥n masiva de problemas.
        
        Muestra c√≥mo generar grandes cantidades de problemas
        para testing exhaustivo.
        """
        print("\n" + "="*70)
        print("  DEMO: Generaci√≥n Masiva de Problemas")
        print("="*70)
        
        factory = ProblemFactory(seed=42)  # Reproducible
        
        # Generar m√∫ltiples tipos de problemas
        configs = []
        
        # N-Reinas: 4 a 12
        for n in range(4, 13):
            configs.append(ProblemConfig(ProblemType.NQUEENS, size=n))
        
        # Graph Coloring: diferentes tama√±os y dificultades
        for size in [5, 10, 15]:
            for difficulty in ["easy", "medium", "hard"]:
                configs.append(ProblemConfig(
                    ProblemType.GRAPH_COLORING,
                    size=size,
                    difficulty=difficulty
                ))
        
        # Scheduling: diferentes configuraciones
        for size in [5, 8, 10]:
            configs.append(ProblemConfig(ProblemType.SCHEDULING, size=size))
        
        # Generar todos los problemas
        problems = factory.create_batch(configs)
        
        print(f"\n‚úÖ Generados {len(problems)} problemas:")
        
        # Contar por tipo
        by_type = {}
        for p in problems:
            category = p.category
            by_type[category] = by_type.get(category, 0) + 1
        
        for category, count in by_type.items():
            print(f"  - {category}: {count} problemas")
        
        assert len(problems) == 9 + 9 + 3  # 21 problemas
        
        print(f"\nüìä Total: {len(problems)} problemas listos para testing")
    
    def test_custom_problem_generation(self):
        """
        Demo: Generaci√≥n de problemas personalizados.
        """
        print("\n" + "="*70)
        print("  DEMO: Problemas Personalizados")
        print("="*70)
        
        factory = ProblemFactory()
        
        # Graph coloring con par√°metros personalizados
        config = ProblemConfig(
            problem_type=ProblemType.GRAPH_COLORING,
            size=10,
            custom_params={
                'num_colors': 3,
                'density': 0.5
            }
        )
        
        problem = factory.create(config)
        
        print(f"\n‚úÖ Problema personalizado creado:")
        print(f"  - {problem.name}")
        print(f"  - Dificultad: {problem.difficulty}")
        print(f"  - Variables: {len(problem.problem.variables)}")
        print(f"  - Restricciones: {len(problem.problem.constraints)}")
        
        assert problem is not None
        assert len(problem.problem.variables) == 10


@pytest.mark.benchmark
def test_factory_integration_example():
    """
    Ejemplo completo: Factory + Runner + Comparaci√≥n.
    
    Muestra un flujo completo de generaci√≥n automatizada,
    ejecuci√≥n de benchmarks y comparaci√≥n de algoritmos.
    """
    print("\n" + "="*70)
    print("  EJEMPLO COMPLETO: Factory + Benchmark + Comparaci√≥n")
    print("="*70)
    
    # 1. Generar problemas autom√°ticamente
    print("\n1Ô∏è‚É£ Generando problemas...")
    problems = batch_problems("nqueens", [4, 6, 8])
    print(f"   ‚úÖ {len(problems)} problemas generados")
    
    # 2. Configurar algoritmos
    print("\n2Ô∏è‚É£ Configurando algoritmos...")
    algorithms = {
        "Backtracking": get_solver("backtracking"),
        "Forward Checking": get_solver("forward_checking"),
    }
    print(f"   ‚úÖ {len(algorithms)} algoritmos listos")
    
    # 3. Ejecutar benchmarks
    print("\n3Ô∏è‚É£ Ejecutando benchmarks...")
    runner = BenchmarkRunner(warmup_runs=1, benchmark_runs=3)
    
    all_results = {}
    for problem in problems:
        print(f"\n   üìä {problem.name}:")
        results = runner.compare_algorithms(problem, algorithms, max_solutions=1)
        all_results[problem.name] = results
        
        # Mostrar resultados
        for algo_name, result in sorted(results.items(), key=lambda x: x[1].time_mean_ms):
            print(f"      {algo_name:<20} {result.time_mean_ms:>7.2f}ms")
    
    # 4. An√°lisis global
    print("\n4Ô∏è‚É£ An√°lisis global:")
    print("-"*70)
    runner.print_summary(baseline_algorithm="Backtracking")
    
    print("\n‚úÖ Flujo completo ejecutado exitosamente")
    
    # Validaciones
    assert len(all_results) == 3
    for problem_results in all_results.values():
        for result in problem_results.values():
            assert result.success_rate == 1.0
            assert result.solutions_found > 0


if __name__ == "__main__":
    # Ejecutar demos directamente
    print("Ejecutando demos de ProblemFactory...")
    pytest.main([__file__, "-v", "-s"])

